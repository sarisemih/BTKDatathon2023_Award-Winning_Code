{"cells":[{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"08a8b51140314edca4e5685bdb7a5853","deepnote_cell_type":"text-cell-h1"},"source":"# DATATHLON 2023 - BTK (Troia's Notebook)","block_group":"608db0f99d164a809181d231c209c72d"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"05a746979050458cbb24e2cce61fc969","deepnote_cell_type":"text-cell-p"},"source":"Contributors: ","block_group":"acede5b0b19646bbb8ab5c74e404ca81"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"b0e211aa250c4690bbf4cee8290e0a96","deepnote_cell_type":"text-cell-p"},"source":"Semih Sarı (4th-year undergraduate student in Computer Engineering at ÇOMÜ)","block_group":"e0c2f1ae5f8045bd9ae3694b048e8fd6"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"bb9b9950f75a4045b0a25bbece349d9d","deepnote_cell_type":"text-cell-p"},"source":"Dr. Ulya Bayram (Assistant Professor @ Department of Electrical and Electronics Engineering, ÇOMÜ).","block_group":"6f8a3935f7e14136afff3e2bb210c7f7"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"75a5956697e84132be843415c1334a6b","deepnote_cell_type":"text-cell-p"},"source":"","block_group":"1cef2e4de83f48b8a4b692c5a9db6453"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"a2d5ea552cba46ce97fc2350b5443410","deepnote_cell_type":"text-cell-p"},"source":"We chose the name \"Troia\" for our group in this competition because we are representing Çanakkale Onsekiz Mart University.","block_group":"03ecc32ed7b2462ebe60882462bf2650"},{"cell_type":"markdown","metadata":{"cell_id":"6b3482dea75b4088a5c23499c2caef8b","deepnote_cell_type":"markdown"},"source":"![TROIA](https://www.getsupport.co.uk/wp-content/uploads/2020/12/trojan-horse.jpg)","block_group":"c04de1ca2a284e0cb07141b6bb7d8ee9"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"url":"https://www.getsupport.co.uk/wp-content/uploads/2020/12/trojan-horse.jpg","type":"link","ranges":[],"toCodePoint":89,"fromCodePoint":17}],"cell_id":"120d7876f3bf456b88c08df114e7e07b","deepnote_cell_type":"text-cell-p"},"source":"Photo reference: https://www.getsupport.co.uk/wp-content/uploads/2020/12/trojan-horse.jpg","block_group":"1275bb1b75ac4b2eaa1545119cc85ee0"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"36a8a91ebd864d3c89f0ba25e4004480","deepnote_cell_type":"text-cell-h3"},"source":"### Required Libraries","block_group":"c199c680b12c4ca28730c264f2e595ba"},{"cell_type":"code","metadata":{"cell_id":"faa6d34ad9ba4da493356605a7fb2206","deepnote_cell_type":"code"},"source":"import joblib\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\n\n!pip install catboost\n!pip install lightgbm\n!pip install xgboost\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 500)\n\nSEED = 17 # License plate number of Çanakkale :)\n\nDIR_ = \"/work/\" # Here we set the directory of the files as a global variable.\n# In addition to Kaggle, we also used Deepnote.com to write common code. \n#So this dir_ will change depending on what environment you are using\n\nTARGET = \"Öbek İsmi\" # Our target (i.e., labels) is in this column","block_group":"88d1de31fec041449d41ad1ac4502495","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"fb4804794cb3499ca642176d60cd7ae7","deepnote_cell_type":"code"},"source":"def prepare_df(df):\n\n    \"\"\"\n        The \"index\" column is being removed by default.\n        It will be needed for both training and testing.\n    \"\"\"\n    \n    df.drop(columns=[\"index\"], axis=1, inplace = True)\n\n    return df\n ","block_group":"2249703a95dc49688ca6df0318e48077","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"9decfb545f8841eda31dd69183ae78cd","deepnote_cell_type":"text-cell-h2"},"source":"## 1) VISUAL DATA ANALYSIS and INTERPRETATION STUDY","block_group":"43fc759ce875489fade29adfd52e20b9"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"7e1161bccd91444db04c05ba9c0ce0ec","deepnote_cell_type":"text-cell-h3"},"source":"### 1.1) Helper Functions Class","block_group":"1619fc0df8ee4e9cb46eba6051e6d653"},{"cell_type":"code","metadata":{"cell_id":"015fa0c37fc648cca3db59f7ddfe67db","deepnote_cell_type":"code"},"source":"class DataframeAnalyzer:\n\n    def __init__(self, dataframe):\n\n        self.df = dataframe\n\n        # also separate the categorical & numerical columns, will be necessary later\n        self.categorical_cols, self.numerical_cols = self.get_column_names()\n\n    def get_column_names(self):\n        \"\"\"\n            We are categorizing the columns as categorical and numerical.\n\n            Output: List of names of categorical columns, list of names of numerical columns.\n        \"\"\"\n        categorical_cols = [col for col in self.df.columns if self.df[col].dtypes == \"O\"]\n        categorical_cols.remove(TARGET)\n\n        numerical_cols = [col for col in self.df.columns if self.df[col].dtypes != \"O\"]\n\n        print(f'Number of categorical_cols: {len(categorical_cols)}')\n        print(f'Number of numerical_cols: {len(numerical_cols)}')\n\n        return categorical_cols, numerical_cols\n    \n    def get_categorical_cols(self):\n        \"\"\"\n            The goal is to provide these categorical columns when requested.\n        \"\"\"\n        return self.categorical_cols\n\n    def get_numerical_cols(self):\n        \"\"\"\n            The goal is to provide these numerical columns when requested.\n        \"\"\"\n        return self.numerical_cols\n\n    def print_df(self):\n        \"\"\"\n            The function's task is to print standard information.\n        \"\"\"\n        \n        print(\"Dataframe Shape:\\n--------------\")\n        print(self.df.shape)\n        \n        print(\"\\nValue Types:\\n-------------\")\n        print(self.df.dtypes)\n        \n        print(\"\\nNumber of null's per column:\\n--------------------\")\n        print(self.df.isnull().sum())\n        \n        print(\"\\nDescriptive Statistics:\\n--------------\")\n        desc_stats = self.df.describe()\n        print(desc_stats.T)\n        \n    def summarize_cat_columns(self):\n        print(\"\\nCategorical Column Summaries:\\n---------------\")\n\n        # Create a sorted list of target names\n        sorted_target_names = sorted(training_df[TARGET].unique())\n\n        for cat_col in self.categorical_cols:\n            print(f\"\\n----- {cat_col} -----\")\n            print(self.df[cat_col].value_counts())\n            print(\"Ratio:\\n------\\n\", self.df[cat_col].value_counts() / len(self.df))\n\n            plt.figure(figsize=(12, 8))\n            happy_colors = ['#FFC300', '#AFEEEE', '#FF0000', '#191970', '#FF1493', '#4CAF50', '#3498DB', '#9B59B6']\n            sns.set_palette(happy_colors)\n            ax = sns.countplot(x=cat_col, data=training_df, hue=TARGET, \n                                hue_order=sorted_target_names, linewidth=1.5, edgecolor=\"black\")\n            # Set vertical gridlines\n            ax.grid(axis='y', linestyle='--', alpha=0.7)\n            plt.title(f\"{cat_col} Distribution\", fontsize=16)\n            plt.xlabel(cat_col, fontsize=15)\n            plt.ylabel(\"Number of people\", fontsize=15)\n            plt.xticks(rotation=45)\n            # Change x and y-axis tick label font size\n            plt.xticks(fontsize=15)\n            plt.yticks(fontsize=15)\n\n            sns.despine()\n            plt.tight_layout()\n\n            # Move legend outside the plot\n            plt.legend(title=TARGET, title_fontsize=18, fontsize=15, bbox_to_anchor=(1.05, 1), loc='upper left')\n\n            plt.show()\n\n    def summarize_num_columns(self):   \n        print(\"\\nNumerical Column Summaries:\\n----------------\")\n\n        # Create a sorted list of target names\n        sorted_target_names = sorted(training_df[TARGET].unique())\n\n        for num_col in self.numerical_cols:\n            print(f\"\\n----- {num_col} -----\")\n            print(\"Min:\", self.df[num_col].min())\n            print(\"Mean:\", self.df[num_col].mean())\n            print(\"Median:\", self.df[num_col].median())\n            print(\"Max:\", self.df[num_col].max())\n            print(\"Std Dev:\", self.df[num_col].std())\n\n            # Set style and palette\n            sns.set(style=\"whitegrid\")\n            happy_colors = ['#FFC300', '#AFEEEE', '#FF0000', '#ADFF2F', '#FF1493', '#4CAF50', '#3498DB', '#9B59B6']\n            sns.set_palette(happy_colors)\n\n            # Create a violin plot\n            plt.figure(figsize=(14, 8))\n            ax = sns.violinplot(x=TARGET, y=num_col, data=self.df, inner=\"quartile\", \n                                linewidth=2, order=sorted_target_names)\n            ax.grid(axis='y', linestyle='--', alpha=0.7)\n\n            # Set title and labels\n            plt.title(f'Distribution of {num_col}', fontsize=16)\n            plt.xlabel(TARGET, fontsize=15)\n            plt.ylabel(num_col, fontsize=15)\n\n            # Change x and y-axis tick label font size\n            plt.xticks(fontsize=15)\n            plt.yticks(fontsize=15)\n\n            # Rotate x-axis labels for better readability\n            plt.xticks(rotation=45, ha='right')\n\n            # Adjust plot layout\n            sns.despine(trim=True)\n            plt.tight_layout()\n\n            # Show the plot\n            plt.show()\n\n\n    def show_correlation_matrix(self):\n        print(\"\\nCorrelation Matrix:\\n-----------------\")\n        corr_matrix = self.df.corr()\n        \n        # Create a heatmap using seaborn\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n        plt.title(\"Correlation Matrix Heatmap of Training\")\n        plt.show()\n    \n    def get_target_summary(self):\n        \"\"\"\n            -It generates a summary report that shows the relationship \n             between the target variable and different categorical and numerical features: \n            -It presents the mean values of the target variable\n             grouped by various categorical and numerical columns (i.e., features).\n\n        \"\"\"\n        print(\"\\n---------- Numerical Columns vs Target (Öbek İsmi) ----------\\n\")\n        print(self.df.groupby(TARGET).agg({self.numerical_cols: \"mean\"}), end=\"\\n\")\n        print(\"\\n---------- Categorical Columns vs Target (Öbek İsmi) ----------\\n\")\n        print(pd.DataFrame({\"TARGET_MEAN\": self.df.groupby(self.categorical_cols)[TARGET].mean()}), end=\"\\n\")\n\n","block_group":"251081d7ff5c4e29877c1a71f6182a83","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"07d2fd28dd714d58b86c8660f229df8f","deepnote_cell_type":"text-cell-h3"},"source":"### 1.2) Statistical and Visual Analysis of Training Data","block_group":"f95f6c3f62bb4c1e83aeffd7163644a7"},{"cell_type":"code","metadata":{"cell_id":"8a4e327ddf4e4bb089ca3763c7cc9b4c","deepnote_cell_type":"code"},"source":"# Starting to read and analyze the training set of the dataset.\ntraining_df = pd.read_csv(DIR_ + \"train.csv\")\ntraining_df = prepare_df(training_df)\n\nprint(training_df.head())\nprint(\"\\n\")\n\nanalyzer = DataframeAnalyzer(training_df)","block_group":"83030c1249f64d8098a9ebaac6eb4cfd","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"60b99ec388864a1faf158edb7b1f43f8","deepnote_cell_type":"code"},"source":"analyzer.print_df()","block_group":"a929bfe680bd4543899259d922b11244","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"c05c82e51fcd448b87c56d5f43aa09bb","deepnote_cell_type":"code"},"source":"# Obtaining categorical and numerical variables\ncategorical_cols = analyzer.get_categorical_cols()\nnumerical_cols = analyzer.get_numerical_cols()","block_group":"8e9c896e34444bf7b14cf8ad69ba54cc","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"9812afc233514ae0830b203c970a127c","deepnote_cell_type":"text-cell-h3"},"source":"### 1.2.1) Analysis of categorical columns","block_group":"652a478c7f294b5e98f0da67aca21cc7"},{"cell_type":"code","metadata":{"cell_id":"62b6dc3015fb4d11a78dad154e9a4026","deepnote_cell_type":"code"},"source":"categorical_cols","block_group":"20cd0b62248b4cc890e0ca2fe424c9d5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"8330986acd5b4e76a2bd681526cd7629","deepnote_cell_type":"code"},"source":"analyzer.summarize_cat_columns()","block_group":"e70d8f7940894e739c17ec0ed4c0e47d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"44e36ff1466d46fabf85cd639aeabb89","deepnote_cell_type":"text-cell-p"},"source":"We have made the following observations based on categorical plots and the number of individuals in each cluster:","block_group":"bfbd0e1ced7b4bc68aca3fad6835c06c"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"fda4166c8fe747a8895a781a2218a8c8","deepnote_cell_type":"text-cell-bullet"},"source":"- Öbek 1: Mostly engaged in clothing and cosmetics shopping, low education level, mostly young below 60.","block_group":"1da6c62554944f509597c35dcae722d6"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"7dd630426c7048fd808a575a29cffb35","deepnote_cell_type":"text-cell-bullet"},"source":"- Öbek 2: Comprised mainly of married women, engaged in clothing and furniture shopping, with a maximum education level of high school, mostly unemployed, and mostly young below 60.","block_group":"64fd7c57469b404496c613849c93c800"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"78a772a1f5be4de1a6b49df91e8723cb","deepnote_cell_type":"text-cell-bullet"},"source":"- Öbek 3: Includes married men of all ages, except for cosmetics, they buy everything, most of them have a high school education, and they don't live in rural areas.","block_group":"206ace3f88de4f168474d4cfb6b37bff"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"3fb117ce0e8545f39afd0d949659d1ed","deepnote_cell_type":"text-cell-bullet"},"source":"- Öbek 4: Single individuals, aged 18-40, living in major cities, at least college graduates, and highly educated, they buy everything.","block_group":"ac0c4b838fa14d87b21703e5ea19855d"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"601e0a75642d47e9a95051c40a6a8a72","deepnote_cell_type":"text-cell-bullet"},"source":"- Öbek 5: Mostly males aged 18-40, high school graduates, not living in rural areas, and not retired.","block_group":"e853183b05b941a2848435366e15c8db"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"4fb379f304f64de6a106c0501ea92fe7","deepnote_cell_type":"text-cell-bullet"},"source":"- Öbek 6: Preferring rural areas, villages, and small towns, mostly retired individuals aged 51 and above, university graduates.","block_group":"633b5b21c7904d558ad061f19933f002"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"e155a262c68a4533afe275e36a33d23b","deepnote_cell_type":"text-cell-bullet"},"source":"- Öbek 7: Living in small and large cities, at least college graduates, with regular jobs, aged 18-50.","block_group":"ef13b71e02f841cab0e4ff7f6238516d"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"e1a8116ba2034c089a0266dbdd7c63c0","deepnote_cell_type":"text-cell-bullet"},"source":"- Öbek 8: The young population continuing their education.","block_group":"899f1dac57354df58de983ea90a76259"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"7e299501ad194f2eb2e6b3b8f5ee0680","deepnote_cell_type":"text-cell-h3"},"source":"### 1.2.2) Analysis of numerical columns","block_group":"aadd28ec39ca410eb2be5ff8ec75eba1"},{"cell_type":"code","metadata":{"cell_id":"b6d753d21e7547ccafb36f02c35e9454","deepnote_cell_type":"code"},"source":"numerical_cols","block_group":"6f4f0718bd894f3981c05d9bb2e73169","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"b39f2f40a04f4f26bfdac39cc86165b6","deepnote_cell_type":"code"},"source":"analyzer.summarize_num_columns()","block_group":"3ac04881b256498d833e34e091b8ed3c","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"9c98b9e7a5f941fca8d2bca5f68b9159","deepnote_cell_type":"text-cell-p"},"source":"In addition to the information obtained from categorical columns, let's add the information from the violin plots of numerical columns:","block_group":"779b90ef2afa40d6b59908d9c232e730"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"e398f967a7b349629c14abe36f2ff47c","deepnote_cell_type":"text-cell-bullet"},"source":"- Öbek 1 (mostly engaged in clothing and cosmetics shopping, low education level, mostly young below 60) is the group with the least products added to the basket and the least products purchased.","block_group":"2577aabf224347aea4de7a4e1a57612e"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"76a21829ef004e389263b6417de95951","deepnote_cell_type":"text-cell-bullet"},"source":"- Öbek 2 (comprised mainly of married women, engaged in clothing and furniture shopping, with a maximum education level of high school, mostly unemployed, and mostly young below 60) has both low income and low spending habits.","block_group":"3d7a5ed6d27b41c59dfb3c761741f212"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"a2d85808147c40b6972bd758b601c8b6","deepnote_cell_type":"text-cell-bullet"},"source":"- Öbek 3 (includes married men of all ages, except for cosmetics, they buy everything, most of them have a high school education, and they don't live in rural areas) adds fewer products to the basket.","block_group":"b679cc6ba7824f3da01aa29d54282eeb"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"e132c134a6ef4f549319f232e6ec7fec","deepnote_cell_type":"text-cell-bullet"},"source":"- Öbek 4 (single individuals, aged 18-40, living in major cities, at least college graduates, and highly educated, they buy everything) has the highest average income annually. Additionally, they purchase a lot of products despite not adding many items to the basket. They must be buying expensive items.","block_group":"cf4dd5869dba4cfbb8adea536ad46b76"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"fee90075eae343439af5f7800a090ce6","deepnote_cell_type":"text-cell-bullet"},"source":"- Öbek 5 (mostly males aged 18-40, high school graduates, not living in rural areas, and not retired) adds a lot to the basket but does not make purchases, and their income is low.","block_group":"e9f5c936eec945a9b14090113938f690"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"8857c83c0fb547a19345fd9f8e7eb5a5","deepnote_cell_type":"text-cell-bullet"},"source":"- Öbek 6 (preferring rural areas, villages, and small towns, mostly retired individuals aged 51 and above, university graduates) is the group that spends the most money.","block_group":"5c51903b5e5e4958b95ba465851f2a48"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"489007f3f92f44c492dbf66de8b4c889","deepnote_cell_type":"text-cell-bullet"},"source":"- Öbek 7 (living in small and large cities, at least college graduates, with regular jobs, aged 18-50) is the third highest spending group.","block_group":"0af2c6181f314e5f8170a51715a81970"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"0f424628665141838762b183a371379b","deepnote_cell_type":"text-cell-bullet"},"source":"- Öbek 8 (the young population continuing their education) is the group that, on average, adds the most products to the basket and makes the most purchases.","block_group":"d53af9ebbb254b76b9dfa732396d7ff1"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"1dfb5e58d8584ed08d96d5b05ded27ac","deepnote_cell_type":"text-cell-h1"},"source":"# 2) Data Preprocessing and Feature Engineering","block_group":"0295fe810de1401495690d3ee40a572c"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"741998f2c6904a118bc1a8f48638bf9a","deepnote_cell_type":"text-cell-p"},"source":"All of these operations have been made functional. Data preparation process has been automated.","block_group":"fb84b0471ffb44e690029912c2d73873"},{"cell_type":"code","metadata":{"cell_id":"92b7a4afa4454da6ab6a1cc31789489d","deepnote_cell_type":"code"},"source":"## Helper Functions Class for Preprocessing\n\nclass CheckForOutliers:\n\n    def __init__(self, dataframe):\n\n        self.df = dataframe\n    \n    def check_outlier(self, col_name):\n        \"\"\"\n        It checks if there are any outliers in the DataFrame based on the specified col_name column. \n        If there are outliers, it returns True; otherwise, it returns False.\n        The determination is based on the threshold values specified in outlier_thresholds.\n        \"\"\"\n        low_limit, up_limit = self.outlier_thresholds(col_name)\n        if self.df[(self.df[col_name] > up_limit) | (self.df[col_name] < low_limit)].any(axis=None):\n            return True\n        else:\n            return False\n\n    def outlier_thresholds(self, col_name, q1=0.05, q3=0.95):\n        \"\"\"\n        The outlier thresholds are defined here to determine outliers.\n        \"\"\"\n        quartile1 = self.df[col_name].quantile(q1)\n        quartile3 = self.df[col_name].quantile(q3)\n        interquantile_range = quartile3 - quartile1\n        up_limit = quartile3 + 1.5 * interquantile_range\n        low_limit = quartile1 - 1.5 * interquantile_range\n\n        return low_limit, up_limit\n","block_group":"b27ba8f1b40841e0b4cb37e395355345","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"284696aef80a4ef1ae208ff40abb2781","deepnote_cell_type":"text-cell-h3"},"source":"### ","block_group":"4b0fcf0d605f4cc5937b8835574c1838"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"ea9cccc47c79486c9a04af7e12e32c75","deepnote_cell_type":"text-cell-h2"},"source":"## 2.1) Outlier Analysis","block_group":"9fe6b3a984284a9e9a2def473204f55f"},{"cell_type":"code","metadata":{"cell_id":"f280b710b738490f97d1d830bed2617b","deepnote_cell_type":"code"},"source":"prep = CheckForOutliers(training_df)\n\nfor col in numerical_cols:\n    print(col, prep.check_outlier(col))\n","block_group":"1ba0533d99684ae0aee770302f85aac0","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"c6becd2de87e40b7a348ca35c12f2f43","deepnote_cell_type":"text-cell-p"},"source":"Since we will be using tree-based methods, higher threshold values have been set. No outliers were found in the numerical features. During model training, no data was discarded in the end.","block_group":"f1a3d4741c9a4b05964d08bba2bc4d8a"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"3585246c5ab84eb79445010ffd5b9a03","deepnote_cell_type":"text-cell-h2"},"source":"## 2.2) Multivariate Outlier Analysis (LOF - Local Outlier Factor)","block_group":"8f64204ced3a4c2f8c9d4330cc8bc789"},{"cell_type":"code","metadata":{"cell_id":"f1547f616bdb4234b269958772f18891","deepnote_cell_type":"code"},"source":"from sklearn.neighbors import LocalOutlierFactor\n\nclf = LocalOutlierFactor(n_neighbors=20)\nclf.fit_predict(training_df[numerical_cols])\ndf_scores = clf.negative_outlier_factor_\nprint(df_scores[0:5])\n\nscores = pd.DataFrame(np.sort(df_scores))\nscores.plot(stacked = True,xlim=[0,50],style='.-')\nplt.show()","block_group":"0786bdf3f15346cca5b912ff2c6a81bc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"9306b7775dd24c59ac0e492c40d055a8","deepnote_cell_type":"code"},"source":"th = np.sort(df_scores)[5]\ntraining_df[numerical_cols][df_scores<th]","block_group":"c4ba2aad24ab4cdc812ef950042427e6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"b580533e3a8b473fb9920be049da12b6","deepnote_cell_type":"text-cell-p"},"source":"We have decided not to use the results of the outlier detection part to avoid losing data.","block_group":"24822707f0a84f1dbae1482f301cea3b"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"a301eb67b64e4a5dbe7151a0e2751168","deepnote_cell_type":"text-cell-h2"},"source":"## 2.3) Feature Engineering efforts yielded positive results for some and negative effects for others. These were identified during model training and performance evaluation.","block_group":"8842d17ea85d48729b12372ba9bb5614"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"3be6fce33dd3406897e3cc1345a4af79","deepnote_cell_type":"text-cell-p"},"source":"We tried adding a large number of new features to existing columns. Our efforts are listed below. Some of them yielded successful results in certain models, while others did so in different models.","block_group":"4bee36dbe96c41c0b0c40a240f78772d"},{"cell_type":"code","metadata":{"cell_id":"369909164aba40b299e2dbcae1891ef6","deepnote_cell_type":"code"},"source":"numerical_cols","block_group":"00b3497bbfc6481a8ffffc44539742b8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"5ae57c23e5b74782b6fb5d461b273596","deepnote_cell_type":"code"},"source":"print(f\"25th percentile value for Annual Average Income: {training_df['Yıllık Ortalama Gelir'].quantile(0.25)}\\n\"\n      f\"75th percentile value for Annual Average Income: {training_df['Yıllık Ortalama Gelir'].quantile(0.75)}\")\n","block_group":"fbce47f5ad2646c995188376d49040b0","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"67c1642eb164470db89f0747e1c6762a","deepnote_cell_type":"text-cell-p"},"source":"%25 and %75 quartile values seem to be sufficient thresholds.","block_group":"ea3c31fe89d84665b500270036d278a2"},{"cell_type":"code","metadata":{"cell_id":"4fd1564147d94d1790076de7083221fd","deepnote_cell_type":"code"},"source":"\ndef get_income_group(gelir):\n\n    \"\"\"\n        We categorize people into 3 groups based on their annual average income.\n        We found these threshold values through the quartile analysis process in the cell above.\n        This new feature has positively influenced the validation performance of the model.\n    \"\"\"\n\n    if gelir < 215693:\n        return \"Düşük Gelir\"\n    elif gelir >= 215693 and gelir < 468188:\n        return \"Orta Gelir\"\n    else:\n        return \"Yüksek Gelir\"\n\ndef gender_age_merge(df):\n    \"\"\"\n        Unlike the previous functions, it combines two categories to create new categories \n        and takes the entire dataframe as input to accomplish this.\n        This new feature has had a positive impact on improving the model's validation performance.\n    \"\"\"\n\n    # A new feature was created using gender and age group. (gender_age)\n    df.loc[(df['Cinsiyet'] == 'Erkek') & (df['Yaş Grubu'] == '18-30'), 'gender_age'] = 'genc_erkek'\n    df.loc[(df['Cinsiyet'] == 'Erkek') & (df['Yaş Grubu'].isin(['31-40', '41-50'])), 'gender_age'] = 'orta_erkek'\n    df.loc[(df['Cinsiyet'] == 'Erkek') & (df['Yaş Grubu'].isin(['51-60', '>60'])), 'gender_age'] = 'yasli_erkek'\n\n    df.loc[(df['Cinsiyet'] == 'Kadın') & (df['Yaş Grubu'] == '18-30'), 'gender_age'] = 'genc_kadin'\n    df.loc[(df['Cinsiyet'] == 'Kadın') & (df['Yaş Grubu'].isin(['31-40', '41-50'])), 'gender_age'] = 'orta_kadin'\n    df.loc[(df['Cinsiyet'] == 'Kadın') & (df['Yaş Grubu'].isin(['51-60', '>60'])), 'gender_age'] = 'yasli_kadin'\n\n    return df\n","block_group":"6be3fca302704a73960a4b0c018075f3","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"df8456c6fe5642cea1803721c9271c51","deepnote_cell_type":"text-cell-h2"},"source":"## 2.4) Encoding, Scaling","block_group":"385fe70e64db4faaa37a691fb7ef5745"},{"cell_type":"code","metadata":{"cell_id":"b715f0a5362f4c059e235cf634e47640","deepnote_cell_type":"code"},"source":"# Functions for Data Preprocessing\n\ndef add_2_new_features(df):\n\n    df[\"NEW_Income_Group\"] = df[\"Yıllık Ortalama Gelir\"].apply(get_income_group)\n    df = gender_age_merge(df)\n\n    return df\n\ndef prepare_files_w_extra_2_feats(DIR_, list_cat_cols, list_numerical_cols):\n    \"\"\"\n    In our validation experiments, an ensemble model with 2 extra features achieved very successful results.\n    Therefore, we wrote a separate function that includes these two extra features, reads and processes the data.\n    This allows us to automate the operations and combine all our notebooks.\n\n    inputs:\n        DIR_               : directory where the files are located.\n        list_cat_cols      : list of categorical column names.\n        list_numerical_cols: list of numerical column names.\n    \"\"\"\n\n    # Let's read the data from scratch because train_df will change in place with each experiment.\n    # We want to read it again each time this function is called to be on the safe side.\n    train = pd.read_csv(DIR_ + \"train.csv\")\n    test = pd.read_csv(DIR_ + \"test_x.csv\")  # reading it for the first time\n\n    train = prepare_df(train)\n    test = prepare_df(test)  # remove the index\n\n    # Target variable transformation (from strings to label numbers)\n    y_labels_str = train[TARGET]\n    conv_str2num_dict, conv_num2str_dict = class_str2num(y_labels_str)\n    y_labels_num = [conv_str2num_dict[str_] for str_ in y_labels_str]\n\n    y_labels_num = np.array(y_labels_num)  # 0,1...7\n    train.drop(TARGET, axis=1, inplace=True)\n\n    # Add two new features to both sets\n    train = add_2_new_features(train)\n    test = add_2_new_features(test)\n\n    list_cat_cols.extend([\"gender_age\", \"NEW_Income_Group\"])\n\n    # Apply One Hot Encoding to both sets\n    train = pd.get_dummies(train, columns=list_cat_cols, drop_first=True)\n    test = pd.get_dummies(test, columns=list_cat_cols, drop_first=True)\n\n    # Standard normalization process for numerical variables\n    scaler = StandardScaler()\n    train[list_numerical_cols] = scaler.fit_transform(train[list_numerical_cols])\n    test[list_numerical_cols] = scaler.transform(test[list_numerical_cols])\n\n    # Total Cluster Count is our class count\n    num_classes = len(set(y_labels_str))\n\n    return train, y_labels_num, y_labels_str, test, num_classes, list_cat_cols\n","block_group":"b5472ff9c17f421e9c3a93fa9e9f9c47","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"6597d30583d4458d92a6bda3109d2bce","deepnote_cell_type":"code"},"source":"# Data Preprocessing and Pre-Modeling Steps\ndef class_str2num(y_labels_str):\n    \"\"\"\n    Function to be used in target variable transformation. In this function, we convert öbek_1 to 0 because some sklearn libraries\n    prefer label values to start from 0. We keep the values as cluster numbers - 1.\n\n    Input: List of string TARGET values like öbek_1, öbek_2, ...\n    Output: Two dictionaries that convert these strings to numbers and vice versa.\n    \"\"\"\n    list_unique = list(set(y_labels_str))\n    conv_str2num_dict = {}\n    conv_num2str_dict = {}\n    \n    for str_ in list_unique:\n        conv_str2num_dict[str_] = int(str_.split(\"_\")[1]) - 1\n        conv_num2str_dict[int(str_.split(\"_\")[1]) - 1] = str_\n    \n    return conv_str2num_dict, conv_num2str_dict\n\ntrain, y_labels_num, y_labels_str, test, num_classes, categorical_cols = prepare_files_w_extra_2_feats(DIR_, categorical_cols, numerical_cols)\n\nprint(train.head(3))\n\nconv_str2num_dict, conv_num2str_dict = class_str2num(y_labels_str)\n","block_group":"b1c023641a6b4c6c82118e98b973d1b4","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"07f1cb0b4bf04ed88f07edcb584a7d74","deepnote_cell_type":"text-cell-h1"},"source":"# 3) Models","block_group":"c894cf1c3c3a48d1ae019581c692bd61"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"6cdfc0c8f9b249509a92777dfa48d85e","deepnote_cell_type":"text-cell-h2"},"source":"## 3.1) First Model","block_group":"536b17ac04764da1ad1db3e0384e4aa0"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"bc4cb3bd4a0548d8ab10a515e3ad367f","deepnote_cell_type":"text-cell-h3"},"source":"### Calling Selected Models","block_group":"c0ec915e87294efe8f0c70c7ce03361e"},{"cell_type":"code","metadata":{"cell_id":"54b7561584aa4cfd92ad91c6d8fe7c23","deepnote_cell_type":"code"},"source":"import warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\n# Ignore UserWarnings and ConvergenceWarnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n# This function evaluates the performance of base models.\n# It evaluates basic classification models on the given X data and y targets.\n# Prints the results to the screen.\ndef base_models(X, y, scoring=\"accuracy\"):\n    print(\"Base Models....\")\n    \n    # A list containing classification models to be evaluated along with their names\n    classifiers = [('LR', LogisticRegression()),\n                   ('KNN', KNeighborsClassifier()),\n                   (\"SVC\", SVC()),\n                   (\"CART\", DecisionTreeClassifier()),\n                   (\"RF\", RandomForestClassifier()),\n                   ('Adaboost', AdaBoostClassifier()),\n                   ('GBM', GradientBoostingClassifier()),\n                   ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n                   ('LightGBM', LGBMClassifier())]\n\n    for name, classifier in classifiers:\n        # Evaluate classifiers with 5-fold cross-validation\n        cv_results = cross_validate(classifier, X, y, cv=5, scoring=scoring)\n        print(f\"{scoring}: {round(cv_results['test_score'].mean(), 4)} ({name}) \")\n\nbase_models(train, y_labels_num)\n","block_group":"20a6b0ef427c4649a4eebbce38e55c13","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"93773066aa734ba28a8fb02ec78c03a4","deepnote_cell_type":"text-cell-p"},"source":"Base models, i.e., commonly used machine learning methods in the literature and competitions, with their default parameters, yield the following 5-fold cross-validation results from the training set:","block_group":"48c9ba5f5e624e029b70d5f9fcbc42f1"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"9b0e2e599b00447ea7fcbc8c36393568","deepnote_cell_type":"text-cell-p"},"source":"accuracy: 0.9474 (LR) \naccuracy: 0.9308 (KNN) \naccuracy: 0.9542 (SVC) \naccuracy: 0.8993 (CART) \naccuracy: 0.9546 (RF) \naccuracy: 0.9218 (Adaboost) \naccuracy: 0.9467 (GBM) \naccuracy: 0.9522 (XGBoost) \naccuracy: 0.9538 (LightGBM) ","block_group":"9eca8544dae441e8a362ddf09f922c20"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"719b3311cc1f4ff7b7bc1756199c12ef","deepnote_cell_type":"text-cell-p"},"source":"The highest cross-validation results were obtained from RF and LightGBM. We are definitely selecting these two.","block_group":"b5fc2a4f484442f3874361221a547a28"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"4914e123b8fc4393af68cb33ffe417b9","deepnote_cell_type":"text-cell-p"},"source":"As the third method (we need an odd number for voting), we have selected KNN. Its results are not higher than the remaining methods, but it has a different algorithmically. According to our hypothesis, if we ensemble this different algorithm model with the two successful methods, we can obtain a model capable of good generalization. This is because, even if we ensemble many tree-based models, it won't make much difference if we use just one RF since the algorithms will keep finding the same things.","block_group":"af6a5386328c450486be0c08a32c9445"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"607fa55f69304b5388d4ac2d247766a7","deepnote_cell_type":"text-cell-p"},"source":"CART and Adaboost were eliminated because they performed poorly. LR was too simple for this ensemble. Since we already have LightGBM, we didn't see the need to add GBM to the ensemble. Here, even though SVC performed well, it was eliminated from the ensemble for two reasons. First, fine-tuning SVC takes much longer compared to others (based on our previous project experiences), and second, we need an odd number of models. Instead of SVC, we chose KNN for its speed and significantly different algorithm. Given that this problem resembles a clustering problem, using KNN in the ensemble seemed more reasonable to us.","block_group":"a39f6694faa5463f8730742ea62fbc02"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"6d0079c3c94c46ec9f81986d55b49035","deepnote_cell_type":"text-cell-h3"},"source":"### Hyperparameter Optimization","block_group":"28a7a23c34ea4829b0eadbe4a9cbf611"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"671a7a8cad8d429ea9efbe2027906a61","deepnote_cell_type":"text-cell-p"},"source":"As a final result, we selected the KNN-RF-LGBM Stacking Ensemble model that performed the best.\n","block_group":"02c6a766a1f04ff6b12f3381ba11c378"},{"cell_type":"code","metadata":{"cell_id":"1c36e73756b5457cb0da2697bef22228","deepnote_cell_type":"code"},"source":"# Bu fonksiyon, GridSearchCV kullanarak sınıflandırma modellerinin hiperparametrelerini optimize eder.\n# Verilen X verileri ve y hedefleri üzerinde modelleri optimize eder.\n# Optimizasyon sonuçlarını ekrana yazdırır.\n# Optimizasyon sonrası en iyi modelleri döndürür.\ndef hyperparameter_optimization(X, y, cv=5, scoring=\"accuracy\"):\n    print(\"Hyperparameter Optimization....\")\n    \n    best_models = {}\n    \n    for name, classifier, params in classifiers:\n        print(f\"########## {name} ##########\")\n        \n        # Çapraz doğrulama ile sınıflandırıcıları değerlendirme (öncesinde)\n        cv_results = cross_validate(classifier, X, y, cv=cv, scoring=scoring)\n        print(f\"{scoring} (Before): {round(cv_results['test_score'].mean(), 4)}\")\n\n        # GridSearchCV ile en iyi parametreleri bulma ve son modeli oluşturma\n        gs_best = GridSearchCV(classifier, params, cv=cv, n_jobs=-1, verbose=False).fit(X, y)\n        final_model = classifier.set_params(**gs_best.best_params_)\n\n        # Çapraz doğrulama ile sınıflandırıcıları değerlendirme (sonrasında)\n        cv_results = cross_validate(final_model, X, y, cv=cv, scoring=scoring)\n        print(f\"{scoring} (After): {round(cv_results['test_score'].mean(), 4)}\")\n        print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n        \n        # En iyi modeli bir sözlüğe ekleyerek kaydetme\n        best_models[name] = final_model\n    \n    return best_models\n\n","block_group":"75b41407e6294491834b333be28c8e8c","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"7633bd95eb6b449b8e45a8cb1678fbcd","deepnote_cell_type":"text-cell-h3"},"source":"### Defining Sets of Parameters for Search","block_group":"6b40f9fc50f94303a68612bb458d6f63"},{"cell_type":"code","metadata":{"cell_id":"4404a52c3aed4f6ab2a01b0b41e44b23","deepnote_cell_type":"code"},"source":"knn_params = {\"n_neighbors\": range(1,50)}\n\nrf_params = {    \n    'max_depth': [None,10,20]\n   }\n\n\nlightgbm_params = {\"learning_rate\": [0.01,0.5],\n                \"n_estimators\": [280,300,500],\n                'objective': ['multiclass'],\n                    'metric': ['multi_logloss'],  # For multiclass classification, 'multi_error' is used with 1 - accuracy\n                    'num_class': [8],           # Number of classes in the dataset\n                    'boosting_type': ['gbdt'],\n                    'num_leaves': [31],\n                    'verbose': [0]}\n\n\n# Models to be Used in Parameter Optimization:\nclassifiers = [\n            ('LightGBM', LGBMClassifier(), lightgbm_params),\n            ('KNN', KNeighborsClassifier(), knn_params),\n            (\"RF\", RandomForestClassifier(), rf_params)\n            ]\n","block_group":"0669de73c86a45bd865e5079caebb509","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"d24768fdac41436db78f4f270e65c4b2","deepnote_cell_type":"code"},"source":"best_models = hyperparameter_optimization(train, y_labels_num)","block_group":"6947d7bb80984d31afee22768d7ec6b7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"4cb9f9a40f0a40fba0778b8a63d336a9","deepnote_cell_type":"code"},"source":"best_models","block_group":"70e267188e6a4cbbae51f6d9586d1c7c","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"6a641406a0aa4cd79ca3cbff49aa3967","deepnote_cell_type":"text-cell-p"},"source":"These are the best parameters. We can continue with these.","block_group":"b8690db14e954e35a9dd50aaa3a81f91"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"0305d1d2af61422599eb64506d04ff54","deepnote_cell_type":"text-cell-h3"},"source":"### Stacking & Ensemble Learning","block_group":"1a19f65d3f5240fa970b85967f04b83f"},{"cell_type":"code","metadata":{"cell_id":"8a91974011e343e18844c6137e5e625d","deepnote_cell_type":"code"},"source":"# This function creates the Voting Classifier model.\n# It creates and evaluates the Voting Classifier using the best models and data provided.\n# Prints the results to the screen.\ndef voting_classifier(best_models, X, y):\n     print(\"Voting Classifier...\")\n     \n     # Create the Voting Classifier using the best models specified\n     voting_clf = VotingClassifier(estimators=[('KNN', best_models[\"KNN\"]), ('RF', best_models[\"RF\"]),\n                                               ('LightGBM', best_models[\"LightGBM\"])],\n                                   voting='soft').fit(X, y)\n     \n     # Evaluate the Voting Classifier's performance with cross-validation\n     cv_results = cross_validate(voting_clf, X, y, cv=5, scoring=[\"accuracy\"])\n\n     print(cv_results)\n     print(\"5-fold cross-validation results:\")\n     print(f\"\\n\\nMean Accuracy: {cv_results['test_accuracy'].mean()}\")\n     print(f\"STD Accuracy: {cv_results['test_accuracy'].std()}\")\n     #print(f\"F1Score: {cv_results['test_f1'].mean()}\")\n     \n     return voting_clf\n","block_group":"8d6bb8af53204d49b1af237c333c6fe1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"is_output_hidden":false,"deepnote_app_is_output_hidden":true,"cell_id":"98fc38c709084d72985730be0e5e3188","deepnote_cell_type":"code"},"source":"# Creating and Saving the Model\nvoting_clf = voting_classifier(best_models, train, y_labels_num)\njoblib.dump(voting_clf, \"lgbm-knn-rf.pkl\")","block_group":"d0138978d76143f38e287480d755d2fd","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"33a7a9327263436da6c3a38c4e624628","deepnote_cell_type":"text-cell-p"},"source":"The mean accuracy obtained from the 5-fold cross-validation on the training set is high, and the standard deviation is sufficiently low. We can apply this ensemble model to the test set.","block_group":"6bffde36e05d40efbbb971b77c1df47d"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"97e07b6e34a4494b81005172917a2202","deepnote_cell_type":"text-cell-h3"},"source":"### Predicting the Test Data and Creating the Submission File","block_group":"bd65334a758848a791b546f8b0f2eae2"},{"cell_type":"code","metadata":{"cell_id":"8b2e46a9bea94836812b6e680ad5ed1b","deepnote_cell_type":"code"},"source":"# Predicting the Test Data\npredicted = voting_clf.predict(test)","block_group":"1ac843058fc64cf3935b6c6b0dfe23bb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"d19fe6958d2d4592a7979a147a6a6abb","deepnote_cell_type":"code"},"source":"def convert_test2str(test_res, conv_num2str_dict):\n    \"\"\"\n  The prediction results obtained from the test set are numbers between 0 and 7. \n  To convert them into the form like öbek_1, öbek_2, etc., we need the dictionary we created in the previous cells.\n    \"\"\"\n    string_list = [conv_num2str_dict[number] for number in test_res]\n    \n    return string_list\n\nsample_submission = pd.read_csv(DIR_ + \"sample_submission.csv\")\nsample_submission = sample_submission.iloc[:1].reindex(range(len(predicted)))\nsample_submission[\"id\"] = range(len(predicted))\nsample_submission[TARGET] = convert_test2str(predicted, conv_num2str_dict)\n\nsample_submission.to_csv('lgbm-knn-rf.csv', index = False) ","block_group":"7d45ebf5ef3045e9a82916955578aa16","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"d57bdaed38b542e1ae3e41033d9c6a2a","deepnote_cell_type":"text-cell-p"},"source":"We don't know if the results are good or bad at this point. The best thing we can do is to obtain results from another ensemble model and compare them. This way, we can perform majority voting on the test set or develop a rule based on the general characteristics of the clusters.","block_group":"06f03bc1f2ea41c4980a78f11dad514e"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"58a680a582d64d8392c7d4ce02f843d8","deepnote_cell_type":"text-cell-h2"},"source":"## 3.2) Second Model","block_group":"f585e677ce3643b5ae6313ae4d574cc7"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"ec9d7df0a11f403db62f8cd89ed1347f","deepnote_cell_type":"text-cell-p"},"source":"It is possible to perform unsupervised voting or ensemble on the test set. This is referred to as \"bagging ensemble\" (L. Breiman, \"Bagging predictors,\" Machine learning, vol. 24, no. 2, pp. 123–140, 1996). To implement this, we decided to keep the models similar and experiment with adding different features. We used our creativity to add more features.","block_group":"febdf7ae7680472f93e1ae500c515bd8"},{"cell_type":"code","metadata":{"cell_id":"8b67b54e8f22453dbf36898efc29e02c","deepnote_cell_type":"code"},"source":"plt.hist(training_df[\"Yıllık Ortalama Satın Alım Miktarı\"])\nplt.title(\"Histogram of Yıllık Ortalama Satın Alım Miktarı\")\nplt.xlabel(\"TL\")\nplt.ylabel(\"Kişi sayısı\")\nplt.show()","block_group":"4d30adc50b084051898b1871d794318c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"95498c079c514958ab474e4ed787a8c1","deepnote_cell_type":"code"},"source":"plt.hist(training_df[\"Yıllık Ortalama Sipariş Verilen Ürün Adedi\"])\nplt.title(\"Histogram of Yıllık Ortalama Sipariş Verilen Ürün Adedi\")\nplt.xlabel(\"Adet\")\nplt.ylabel(\"Kişi sayısı\")\nplt.show()","block_group":"5d4c5526cae540d4954340aa0819e4f6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"3c5765c6ef3b4e17b0451ccf96359a09","deepnote_cell_type":"code"},"source":"plt.hist(training_df[\"Yıllık Ortalama Sepete Atılan Ürün Adedi\"])\nplt.title(\"Histogram of Yıllık Ortalama Sepete Atılan Ürün Adedi\")\nplt.xlabel(\"Adet\")\nplt.ylabel(\"Kişi sayısı\")\nplt.show()","block_group":"c089096b97054c3981f12257634799df","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"205d54784bc3466db8684a0655ca23da","deepnote_cell_type":"code"},"source":"def satinalim_grubu(val):\n    \"\"\"\n    We grouped people based on how much they spend annually on purchasing products.\n    We found the threshold values by examining the average values.\n    \"\"\"\n\n    if val < 5000:\n        return \"çok düşük\"\n    elif val >= 5000 and val < 10000:\n        return \"az düşük\"\n    elif val >= 10000 and val < 25000:\n        return \"orta\"\n    elif val >= 25000 and val < 40000:\n        return \"yüksek\"\n    else:\n        return \"çok yüksek\"\n\ndef alisveriskolik_grubu(val):\n    \"\"\"\n    To group based on the values in the \"Annual Average Number of Products Ordered\" column,\n    we wrote this function. We also selected these values by examining the histogram.\n    \"\"\"\n\n    if val < 10:\n        return \"çok az\"\n    elif val >= 10 and val < 20:\n        return \"az\"\n    elif val >= 20 and val < 30:\n        return \"normal\"\n    elif val >= 30 and val < 40:\n        return \"çok\"\n    elif val >= 40 and val < 50:\n        return \"çok çok\"\n    else:\n        return \"abartma\"\n\ndef acgozlu_grubu(val):\n    \"\"\"\n    To group based on the values in the \"Annual Average Number of Products Added to Cart\" column,\n    we determined the threshold values from the histogram.\n    We called those who added items to their cart greedy, but actually it would be more accurate to say enthusiastic.\n    After all, how many of us keep items in the cart and don't remove them after seeing the total cost?\n    \"\"\"\n\n    if val < 50:\n        return \"az\"\n    elif val >= 50 and val < 100:\n        return \"normal\"\n    elif val >= 100 and val < 150:\n        return \"çok\"\n    else:\n        return \"acgozlu\"\n\ndef add_more_features(df):\n\n    # Create a new feature called \"NEW_Satin_Alim_Grubu\" by applying the \"satinalim_grubu\" function\n    # to the \"Yıllık Ortalama Satın Alım Miktarı\" column based on how much they spend annually on purchasing products.\n    df[\"NEW_Satin_Alim_Grubu\"] = df[\"Yıllık Ortalama Satın Alım Miktarı\"].apply(satinalim_grubu).tolist()\n\n    # Create a new feature called \"NEW_Alisveris_Grubu\" by applying the \"alisveriskolik_grubu\" function\n    # to the \"Yıllık Ortalama Sipariş Verilen Ürün Adedi\" column based on the number of products ordered annually.\n    df[\"NEW_Alisveris_Grubu\"] = df[\"Yıllık Ortalama Sipariş Verilen Ürün Adedi\"].apply(alisveriskolik_grubu).tolist()\n\n    # Create a new feature called \"NEW_Acgozlu_Grubu\" by applying the \"acgozlu_grubu\" function\n    # to the \"Yıllık Ortalama Sepete Atılan Ürün Adedi\" column based on the number of products added to the cart annually.\n    df[\"NEW_Acgozlu_Grubu\"] = df[\"Yıllık Ortalama Sepete Atılan Ürün Adedi\"].apply(acgozlu_grubu).tolist()\n\n    age_dict = {\"18-30\": 24.0, \"31-40\": 35.0, \"41-50\": 45.0, \"51-60\": 55.0, \">60\": 65.0}\n    df[\"NEW_Age\"] = [age_dict[str_] for str_ in df[\"Yaş Grubu\"].tolist()]\n\n    return df\n","block_group":"c20db2cc6bf14a6592546e72061fe176","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"6f70349f6223493d84e9f3ecff29d1e1","deepnote_cell_type":"code"},"source":"def prepare_files_more_feats(DIR_, list_cat_cols, list_numerical_cols):\n    \"\"\"\n    A copy of prepare_files_w_extra_2_feats(DIR_, list_cat_cols, list_numerical_cols)\n    We are just trying to add more new features for our validation experiments.\n\n    Inputs:\n        DIR_               : Directory where the files are located.\n        list_cat_cols      : List of categorical column names.\n        list_numerical_cols: List of numerical column names.\n    \"\"\"\n\n    # Reading the data from scratch\n    train = pd.read_csv(DIR_ + \"train.csv\")\n    test = pd.read_csv(DIR_ + \"test_x.csv\")\n\n    train = prepare_df(train)\n    test = prepare_df(test) # Remove the index from this\n\n    # Transformation of the target (TARGET) variable (from String to label numbers)\n    y_labels_str = train[TARGET]\n    conv_str2num_dict, conv_num2str_dict = class_str2num(y_labels_str)\n    y_labels_num = [conv_str2num_dict[str_] for str_ in y_labels_str]\n\n    y_labels_num = np.array(y_labels_num)  # 0,1...7\n    train.drop(TARGET, axis=1, inplace=True)\n\n    # Add two new features to both sets\n    train = add_more_features(train)\n    test = add_more_features(test)\n\n    list_cat_cols.extend([\"NEW_Satin_Alim_Grubu\", \"NEW_Alisveris_Grubu\", \"NEW_Acgozlu_Grubu\"])\n    list_numerical_cols.extend([\"NEW_Age\"])\n\n    # One Hot Encoding\n    train = pd.get_dummies(train, columns=list_cat_cols, drop_first=True)\n    test = pd.get_dummies(test, columns=list_cat_cols, drop_first=True)\n\n    # Standardization for numerical variables\n    scaler = StandardScaler()\n    train[list_numerical_cols] = scaler.fit_transform(train[list_numerical_cols])\n    test[list_numerical_cols] = scaler.transform(test[list_numerical_cols])\n\n    # Total number of clusters\n    num_classes = len(set(y_labels_str))\n\n    return train, y_labels_num, y_labels_str, test, num_classes, list_cat_cols, list_numerical_cols\n","block_group":"6550bc4711e94bdf9afcd6f0661bce64","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"8d79c51fdf6b408987f17aec093f2842","deepnote_cell_type":"code"},"source":"# Data Preprocessing and Pre-Model Processing\ntraining_df = pd.read_csv(DIR_ + \"train.csv\")\ntraining_df = prepare_df(training_df)\n\nanalyzer = DataframeAnalyzer(training_df)\n\ncategorical_cols = analyzer.get_categorical_cols()\nnumerical_cols = analyzer.get_numerical_cols()\n\ntrain, y_labels_num, y_labels_str, test, num_classes, list_cat_cols, list_numerical_cols = prepare_files_more_feats(DIR_, categorical_cols, numerical_cols)\ntrain.head(3)","block_group":"1412be9dd686493b811ba4d69e221d33","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"b58cf23878ef41c0b2309ba5e18cbe28","deepnote_cell_type":"code"},"source":"knn_params = {\"n_neighbors\": [8]}\n\nrf_params = {\"max_depth\": [8],\n             \"max_features\": [8],\n             \"min_samples_split\": [15],\n             \"n_estimators\": [300]}\n\n\nlightgbm_params = {\"learning_rate\": [0.01],\n                   \"n_estimators\": [300],\n                   \"colsample_bytree\": [0.7, 1],\n                   'objective': ['multiclass'],\n                    'metric': ['multi_logloss'],  # For multiclass classification, 'multi_error' is used with 1 - accuracy\n                    'num_class': [num_classes],           # Number of classes in the dataset\n                    'boosting_type': ['gbdt'],\n                    'num_leaves': [31],\n                    'feature_fraction': [0.9],\n                    'bagging_fraction': [0.8],\n                    'bagging_freq': [5],\n                    'verbose': [0]}\n\nclassifiers = [('KNN', KNeighborsClassifier(), knn_params),\n               #(\"CART\", DecisionTreeClassifier(), cart_params),\n               (\"RF\", RandomForestClassifier(), rf_params),\n               #('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgboost_params),\n               ('LightGBM', LGBMClassifier(), lightgbm_params)]\n\ndef base_models(X, y, scoring=\"accuracy\"):\n    print(\"Base Models....\")\n    \n    # A list containing the names of classification models to be evaluated.\n    classifiers = [('LR', LogisticRegression()),\n                   ('KNN', KNeighborsClassifier()),\n                   (\"SVC\", SVC()),\n                   (\"CART\", DecisionTreeClassifier()),\n                   (\"RF\", RandomForestClassifier()),\n                   ('Adaboost', AdaBoostClassifier()),\n                   ('GBM', GradientBoostingClassifier()),\n                   ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n                   ('LightGBM', LGBMClassifier()),\n                   # ('CatBoost', CatBoostClassifier(verbose=False))\n                   ]\n\n    for name, classifier in classifiers:\n        # Evaluating classifiers with cross-validation.\n        cv_results = cross_validate(classifier, X, y, cv=5, scoring=scoring)\n        print(f\"{scoring}: {round(cv_results['test_score'].mean(), 4)} ({name}) \")\n\nbase_models(train, y_labels_num)\n\n","block_group":"faaec7d7caa44d728cbb65ca9cb34fde","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"c73a934072a847fe8055a5091309c66f","deepnote_cell_type":"code"},"source":"best_models = hyperparameter_optimization(train, y_labels_num)\n\nvoting_clf_new_feature = voting_classifier(best_models, train, y_labels_num)\njoblib.dump(voting_clf_new_feature, \"voting_clf_new_feature.pkl\")\n","block_group":"566054a3af204b1a9246617aac27ea37","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"166b33876cd648908193cde60b352ae7","deepnote_cell_type":"text-cell-p"},"source":"\"Mean accuracy\" is almost the same as the previous ensemble model, with a change starting from the fourth decimal place. Let's see how it affects the test set on how many samples it becomes evident:","block_group":"dbbeaaaf6c7b4554b3b09b84392bb6f7"},{"cell_type":"code","metadata":{"cell_id":"c5b287db02a049209c03627f204947d0","deepnote_cell_type":"code"},"source":"predicted=voting_clf_new_feature.predict(test)\n\nsample_submission = pd.read_csv(DIR_+\"sample_submission.csv\")\nsample_submission = sample_submission.iloc[:1].reindex(range(len(predicted)))\nsample_submission[\"id\"] = range(len(predicted))\nsample_submission[TARGET] = convert_test2str(predicted, conv_num2str_dict)\n\nsample_submission.to_csv('new_feature_lgbm-knn-rf.csv', index = False) \nsample_submission\n","block_group":"ab343da928d24331a883833108407342","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"51914c23dbf441eb8a9f106769fc76eb","deepnote_cell_type":"text-cell-h2"},"source":"## 3.3) Third Model","block_group":"03b8374c0b7246dd83a192522c3e61a5"},{"cell_type":"code","metadata":{"cell_id":"ca9f8b26b7a54bfba840e801cdc04a7b","deepnote_cell_type":"code"},"source":"def add_much_more_features(df):\n    df[\"NEW_Ortalama_TekUrun_Fiyati\"] = df[\"Yıllık Ortalama Satın Alım Miktarı\"] / (df[\"Yıllık Ortalama Sipariş Verilen Ürün Adedi\"]  + 0.0001)\n    df[\"NEW_Greed\"] = df[\"Yıllık Ortalama Sepete Atılan Ürün Adedi\"] / (df[\"Yıllık Ortalama Sipariş Verilen Ürün Adedi\"] + 0.0001)\n    df[\"NEW_Greed2\"] = df[\"Yıllık Ortalama Sepete Atılan Ürün Adedi\"] - df[\"Yıllık Ortalama Sipariş Verilen Ürün Adedi\"]\n    df[\"NEW_Savings\"] = df['Yıllık Ortalama Gelir'] - df['Yıllık Ortalama Satın Alım Miktarı']\n    df[\"NEW_Aylik_Gelir\"] = df['Yıllık Ortalama Gelir']/12.0\n    df[\"NEW_Aylik_Satin_Alim\"] = df[\"Yıllık Ortalama Satın Alım Miktarı\"] / 12.0\n\n    df[\"NEW_Gelir_Grubu\"] = df[\"Yıllık Ortalama Gelir\"].apply(get_income_group)\n    df = gender_age_merge(df)\n    df[\"NEW_Satin_Alim_Grubu\"] = df[\"Yıllık Ortalama Satın Alım Miktarı\"].apply(satinalim_grubu).tolist()\n    df[\"NEW_Alisveris_Grubu\"] = df[\"Yıllık Ortalama Sipariş Verilen Ürün Adedi\"].apply(alisveriskolik_grubu).tolist()\n    df[\"NEW_Acgozlu_Grubu\"] = df[\"Yıllık Ortalama Sepete Atılan Ürün Adedi\"].apply(acgozlu_grubu).tolist()\n    # Calculate \"NEW_Aylik_Satin_Alim\" as the monthly average purchase amount by dividing the \"Yıllık Ortalama Satın Alım Miktarı\" value.\n\n    age_dict = {\"18-30\": 24.0, \"31-40\": 35.0, \"41-50\": 45.0, \"51-60\": 55.0, \">60\": 65.0}\n\n    df[\"NEW_Age\"] = [age_dict[str_] for str_ in df[\"Yaş Grubu\"].tolist()]\n\n    return df\n\ndef prepare_files_more_x_feats(DIR_, list_cat_cols, list_numerical_cols):\n    \"\"\"\n    This function can be considered as a copy of prepare_files_w_extra_2_feats(DIR_, list_cat_cols, list_numerical_cols).\n\n    We are only trying to add more new features to our validation experiments.\n\n    Inputs:\n        DIR_               : The directory where the files are located.\n        list_cat_cols      : List of categorical column names.\n        list_numerical_cols: List of numerical column names.\n    \"\"\"\n\n    # Reading the data from scratch\n    train = pd.read_csv(DIR_ + \"train.csv\")\n    test = pd.read_csv(DIR_ + \"test_x.csv\")\n\n    train = prepare_df(train)\n    test = prepare_df(test) # Remove the index from this as well\n\n    # Transforming the target variable (from string labels to label numbers)\n    y_labels_str = train[TARGET]\n    conv_str2num_dict, conv_num2str_dict = class_str2num(y_labels_str)\n    y_labels_num = [conv_str2num_dict[str_] for str_ in y_labels_str]\n    \n    y_labels_num = np.array(y_labels_num)  # 0,1...7\n    train.drop(TARGET, axis=1, inplace=True)\n\n    # Adding two new features to both sets\n    train = add_much_more_features(add_2_new_features(train))\n    test = add_much_more_features(add_2_new_features(test))\n    \n    list_cat_cols.extend([\"NEW_Satin_Alim_Grubu\", \"NEW_Alisveris_Grubu\", \"NEW_Acgozlu_Grubu\", \n                            \"NEW_Gelir_Grubu\", \"gender_age\"])\n    list_numerical_cols.extend([\"NEW_Ortalama_TekUrun_Fiyati\", \"NEW_Greed\", \"NEW_Greed2\", \"NEW_Savings\",\n                                \"NEW_Aylik_Gelir\", \"NEW_Aylik_Satin_Alim\", \"NEW_Age\"])\n\n    # One Hot Encoding\n    train = pd.get_dummies(train, columns=list_cat_cols, drop_first=True)\n    test = pd.get_dummies(test, columns=list_cat_cols, drop_first=True)\n \n    # Standardization for numerical variables\n    scaler = StandardScaler()\n    train[list_numerical_cols] = scaler.fit_transform(train[list_numerical_cols])\n    test[list_numerical_cols] = scaler.transform(test[list_numerical_cols])\n\n    # Total number of clusters\n    num_classes = len(set(y_labels_str))\n\n    return train, y_labels_num, y_labels_str, test, num_classes, list_cat_cols, list_numerical_cols\n\n\n","block_group":"6f40c999bbfe4e6d8668cdd76eb2e4d3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"27a6d153bd91419f8a68bda4158c2e87","deepnote_cell_type":"code"},"source":"training_df = pd.read_csv(DIR_ + \"train.csv\")\ntraining_df = prepare_df(training_df)\n\nanalyzer = DataframeAnalyzer(training_df)\n\ncategorical_cols = analyzer.get_categorical_cols()\nnumerical_cols = analyzer.get_numerical_cols()\n\ncategorical_cols = analyzer.get_categorical_cols()\nnumerical_cols = analyzer.get_numerical_cols()\n\ntrain, y_labels_num, y_labels_str, test, num_classes, list_cat_cols, list_numerical_cols = prepare_files_more_x_feats(DIR_, categorical_cols, numerical_cols)\ntrain.head(3)","block_group":"83e570b682204bf28d6dafef5312adb1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"3e1d5be0d41f464fbdae666242bec4c2","deepnote_cell_type":"code"},"source":"knn_params = {\"n_neighbors\": [8]}\n\nrf_params = {\"max_depth\": [8],\n             \"max_features\": [8],\n             \"min_samples_split\": [15],\n             \"n_estimators\": [300]}\n\n\nlightgbm_params = {\"learning_rate\": [0.01],\n                   \"n_estimators\": [300],\n                   \"colsample_bytree\": [0.7, 1],\n                   'objective': ['multiclass'],\n                    'metric': ['multi_logloss'],  # For multiclass classification, 'multi_error' is used with 1 - accuracy\n                    'num_class': [num_classes],           # Number of classes in the dataset\n                    'boosting_type': ['gbdt'],\n                    'num_leaves': [31],\n                    'feature_fraction': [0.9],\n                    'bagging_fraction': [0.8],\n                    'bagging_freq': [5],\n                    'verbose': [0]}\n\nclassifiers = [('KNN', KNeighborsClassifier(), knn_params),\n               #(\"CART\", DecisionTreeClassifier(), cart_params),\n               (\"RF\", RandomForestClassifier(), rf_params),\n               #('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgboost_params),\n               ('LightGBM', LGBMClassifier(), lightgbm_params)]\n\ndef base_models(X, y, scoring=\"accuracy\"):\n    print(\"Base Models....\")\n    \n    # A list containing the names of classification models to be evaluated.\n    classifiers = [('LR', LogisticRegression()),\n                   ('KNN', KNeighborsClassifier()),\n                   (\"SVC\", SVC()),\n                   (\"CART\", DecisionTreeClassifier()),\n                   (\"RF\", RandomForestClassifier()),\n                   ('Adaboost', AdaBoostClassifier()),\n                   ('GBM', GradientBoostingClassifier()),\n                   ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n                   ('LightGBM', LGBMClassifier()),\n                   # ('CatBoost', CatBoostClassifier(verbose=False))\n                   ]\n\n    for name, classifier in classifiers:\n        # Evaluating classifiers with cross-validation.\n        cv_results = cross_validate(classifier, X, y, cv=5, scoring=scoring)\n        print(f\"{scoring}: {round(cv_results['test_score'].mean(), 4)} ({name}) \")\n\nbase_models(train, y_labels_num)\n\n","block_group":"d62d875c67a749af875ae2d10ce14ea8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"cfc5d09271e44644a800fb937383ffbd","deepnote_cell_type":"code"},"source":"best_models = hyperparameter_optimization(train, y_labels_num)\n\nvoting_clf_new_feature = voting_classifier(best_models, train, y_labels_num)\njoblib.dump(voting_clf_new_feature, \"voting3_clf_new_feature.pkl\")\n","block_group":"50c363dbadce4474aac5985e7606a2b8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"3744bef6451d4207931bc864998bb2cd","deepnote_cell_type":"code"},"source":"predicted=voting_clf_new_feature.predict(test)\n\nsample_submission = pd.read_csv(DIR_+\"sample_submission.csv\")\nsample_submission = sample_submission.iloc[:1].reindex(range(len(predicted)))\nsample_submission[\"id\"] = range(len(predicted))\nsample_submission[TARGET] = convert_test2str(predicted, conv_num2str_dict)\n\nsample_submission.to_csv('new_extra_feature_lgbm-knn-rf.csv', index = False) \nsample_submission\n","block_group":"44b0bc8f3b4a4017a7e1dac735ccc720","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"cell_id":"a8a448df41dd404a87b2f8478e121867","deepnote_cell_type":"text-cell-h2"},"source":"## 4) Bagging Ensemble & Human-in-the-loop","block_group":"b43d34e1319e4c32a2aadc976763c91c"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"3fe06a7fc4b34255ae0f3592798db362","deepnote_cell_type":"text-cell-p"},"source":"This ensemble is entirely unsupervised. Let's compare the results of the two test sets and see how much difference there is between them:","block_group":"6f6923c8c2ef4382ae6909ccfd1eaaa0"},{"cell_type":"code","metadata":{"cell_id":"a6d4aa06f1d24309825edd49b37cc3c6","deepnote_cell_type":"code"},"source":"def get_different_samples(df1, df2):\n    different_filter = df1 != df2\n    a = pd.DataFrame()\n    \n    for column in df1.columns:\n        different_values_in_column = different_filter[column]\n        a[f\"1.DF - {column}\"] = df1[column][different_values_in_column]\n        a[f\"2.DF - {column}\"] = df2[column][different_values_in_column]\n        \n    return a \n\nmodel1 = pd.read_csv(\"lgbm-knn-rf.csv\")\nmodel2 = pd.read_csv('new_feature_lgbm-knn-rf.csv')\n\ndiff_df = get_different_samples(df1, df2)\n\ndiff_df","block_group":"286e92113d004ceb8f7162883b3cc8eb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"8aec5cdbe9a54f178c1d9a81610914fb","deepnote_cell_type":"code"},"source":"def get_different_samples(df1, df2):\n    different_filter = df1 != df2\n    a = pd.DataFrame()\n    \n    for column in df1.columns:\n        different_values_in_column = different_filter[column]\n        a[f\"1.DF - {column}\"] = df1[column][different_values_in_column]\n        a[f\"2.DF - {column}\"] = df2[column][different_values_in_column]\n        \n    return a \n\nmodel1 = pd.read_csv(\"lgbm-knn-rf.csv\")\nmodel3 = pd.read_csv('new_extra_feature_lgbm-knn-rf.csv')\n\ndiff_df = get_different_samples(df1, df2)\n\ndiff_df","block_group":"06e9244ad42f4e57a19e3dc22dc29719","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"7e3e8d710ae94d2d8627e93c9c5d4a5a","deepnote_cell_type":"code"},"source":"def get_different_samples(df1, df2):\n    different_filter = df1 != df2\n    a = pd.DataFrame()\n    \n    for column in df1.columns:\n        different_values_in_column = different_filter[column]\n        a[f\"1.DF - {column}\"] = df1[column][different_values_in_column]\n        a[f\"2.DF - {column}\"] = df2[column][different_values_in_column]\n        \n    return a \n\nmodel2 = pd.read_csv(\"new_feature_lgbm-knn-rf.csv\")\nmodel3 = pd.read_csv('new_extra_feature_lgbm-knn-rf.csv')\n\ndiff_df = get_different_samples(df1, df2)\n\ndiff_df","block_group":"0813d2360a4d4811bbaf54a4e5fa1f3b","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"bf86953b1a3f4ed29d1f2ed48fa21b89","deepnote_cell_type":"text-cell-p"},"source":"When the cross-validation results of the two classifiers are almost identical, there are differences in the classification of 5 samples in the test results. According to majority voting, sample number 1787 is actually classified as \"obek_2\" by 2 out of 3 models. Therefore, we will eliminate it as \"obek_2\".","block_group":"f6428a1ca3cc4afe89ea5656217de1bd"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"57646683389e41ce812ec87484e9d252","deepnote_cell_type":"text-cell-p"},"source":"The fact that the classification results of \"new_feature_lgbm-knn-rf\" and \"new_extra_feature_lgbm-knn-rf\" methods are 100% identical doesn't convince us due to the high similarity of features.","block_group":"0164c5abf1fe4461aae478c7bb4cf6f3"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"2b19153bb5e44dd88b1a47cfefb31b24","deepnote_cell_type":"text-cell-p"},"source":"Interestingly, while the first model calls what the second model refers to as \"obek_1\" as \"obek_3\", it calls what the second model refers to as \"obek_2\" as \"obek_5\". \"obek_1\" and \"obek_3\", as well as \"obek_2\" and \"obek_5\", are mixed up. Here are the descriptions of the clusters we had made earlier:","block_group":"6a4b822435ae47b485f1d4398faa2b0a"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"3da18fa409854ad1b70612f4a8ccb251","deepnote_cell_type":"text-cell-bullet"},"source":"- Obek 1 (mostly engaged in clothing and cosmetics shopping, low educational level, under 60 years old): This group adds the fewest products to the cart and makes the fewest purchases.","block_group":"d05aff4257eb413b9e89d5841c22ed94"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"4f9517e903434e4db86f1504173697ed","deepnote_cell_type":"text-cell-bullet"},"source":"- Obek 3 (includes married men of all ages, they buy everything except cosmetics, most of them are high school graduates, they mostly do not live in rural areas): They add few products to the cart.","block_group":"422755588dd64b1bacefc8cfa05d8e9e"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"d27bc80a77244f189e30e59347d5ffc1","deepnote_cell_type":"text-cell-bullet"},"source":"- Obek 2 (mostly consists of married women, they shop for clothing and furniture, the highest educational level is usually high school, mostly unemployed, under 60 years old): They have both low income and low spending.","block_group":"57be103834434df9b36e8d15012eb667"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"728b3c88dfaf4c8aa3108243837f63a4","deepnote_cell_type":"text-cell-bullet"},"source":"- Obek 5 (mostly males aged 18-40, high school graduates, not living in rural areas, none of them are retired): They add a lot to the cart but don't make purchases, and they have low income.","block_group":"4726f04c298e43ebb2fae31820ce60f3"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"97fe0db14f6a4dbcbf0bd67299c83604","deepnote_cell_type":"text-cell-p"},"source":"Let's see the contents in the test set:","block_group":"7d9f5affbde54866865fe1776d250ad0"},{"cell_type":"code","metadata":{"cell_id":"08a3d4b8dfe94cb38ad866d32661f362","deepnote_cell_type":"code"},"source":"test_raw = pd.read_csv(DIR_ + \"test_x.csv\")\n\n# Let's make the final submission a copy of model1, which provided the highest 5-fold cross-validation result in the validation set.\nfinal_submission = model1.copy()","block_group":"ab9d9283e0504928bb2e2582b46eae1d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"e2db7ea2763a4054bc5d1408e993094d","deepnote_cell_type":"code"},"source":"test_raw.iloc[158] #seems to be a sample where the model had some difficulty determining whether it belongs \n#to cluster 1 or cluster 3. Considering the categorical information, it might not make sense for this individual to belong to both groups.\n# To make a decision, you analyzed the numerical columns, particularly focusing on the \"Yıllık Ortalama Sipariş Verilen Ürün Adedi\" \n#feature.\n#In this case, the \"Yıllık Ortalama Sipariş Verilen Ürün Adedi\" value is around 15,\n#which aligns more with cluster 3 where the average is about 15.\n#On the other hand, cluster 1 has an average value of around 10-12 and does not go higher. \n#Considering this numerical feature, it would be reasonable to classify this sample as belonging to cluster 3.\n\n#So, based on the analysis, you decided to rely on the result of Model1 for this specific sample.","block_group":"4b52d45d317f4729be11668f18a538bc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"78a05015536648e9a9672b478d18321c","deepnote_cell_type":"code"},"source":"print(test_raw.iloc[866]) # Mixed between cluster 1 and 3.\n# Cluster 1 was a younger group, while in cluster 3, the distribution of different age groups seems to be higher compared to cluster 1.\n# The number of items added to the cart is completely equal to the number of items ordered, indicating that the person buys what they put in the cart.\n# At the same time, the order counts are low. It's challenging to classify this person. Additional analysis is needed.\n\ntrain_raw = pd.read_csv(DIR_ + \"train.csv\")\nprint(train_raw[(train_raw[\"Age Group\"] == \">60\") & \n     (train_raw[\"Gender\"] == \"Male\") & \n     (train_raw[TARGET] == \"cluster_1\") & \n     (~train_raw[\"Education Level\"].isin([\"Master's Degree\", \"Bachelor's Degree\", \"Postgraduate\", \n                                        \"Doctorate\", \"College Graduate\"]))])\n\nprint(\"\\n\")\nprint(train_raw[(train_raw[\"Age Group\"] == \">60\") & \n     (train_raw[\"Gender\"] == \"Male\") & \n     (train_raw[TARGET] == \"cluster_3\") & \n     (~train_raw[\"Education Level\"].isin([\"Master's Degree\", \"Bachelor's Degree\", \"Postgraduate\", \n                                        \"Doctorate\", \"College Graduate\"]))])\n\n# Based on the analysis, in terms of age, gender, and education level criteria, there were 90 people in cluster 3 and 15 people in cluster 1.\n# We can only label this person with the result of Model 2 or Model 3.\nfinal_submission[TARGET][866] = model2[TARGET][866]\n","block_group":"8025327b9a274a56baaf074ca63da40e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"768a426c95bc496e8a13f7d30c3b203f","deepnote_cell_type":"code"},"source":"test_raw.iloc[977]\n# Mixed between cluster_2 and cluster_5.\n# (cluster_2 mainly consisted of unemployed women who shopped for clothing, similar to housewives.\n# cluster_5, on the other hand, was predominantly male.\n# This person should belong to cluster_2. I have more confidence in the Model1 result here.)\n","block_group":"b450a633ee1149549f992d594ba23ec6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"37811b5096a24d1585df87b627b10ba7","deepnote_cell_type":"code"},"source":"test_raw.iloc[1332]\n# Mixed between cluster_1 and cluster_3.\n# (cluster_1 was buying cosmetics, while cluster_3 was not. This should belong to cluster_1. I trust Model1's result in this example.)\n","block_group":"18179e9c58674c2297ec3f452f39dab8","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"5a25da3c024b4f17aa94d869a0a75835","deepnote_cell_type":"text-cell-p"},"source":"In the human-intervention process, we have only changed the result of a single data point with the result of Model2 or Model3 based on the Model1 results. Now, we can save this file and prepare it for submission:","block_group":"44cf8f6e976f4bd7a9253e1b8a8fb901"},{"cell_type":"code","metadata":{"cell_id":"10bcd5176a8846b29bec4f2e3e69957a","deepnote_cell_type":"code"},"source":"final_submission.to_csv(\"Ensamble-rf-knn-lgbm.csv\", index=False)\n\n# With this result, Score in the public test set: 0.96195\n# Private score on private test set: 0.95200","block_group":"9d44f52be23c494893d5009fe432c87b","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=88d435bb-04e3-4b54-a36b-59ced22d48e2' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"de864239cb9b428ea26c80d37380ca66","deepnote_execution_queue":[]}}